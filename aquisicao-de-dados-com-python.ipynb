{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aquisição de Dados com Python\n",
    "\n",
    "**Conteúdo:**\n",
    "- O que não te contaram sobre análise de dados\n",
    "- Web scraping\n",
    "- BeautifulSoup\n",
    "- Requests\n",
    "- Páginas dinâmicas\n",
    "- Extração de dados de arquivos de Word\n",
    "- Extração de dados de arquivos PDF\n",
    "- Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O que não te contaram sobre análise de dados\n",
    "<img src=\"https://i.imgur.com/iX7RLQm.png\">\n",
    "<center>Banko, M., Brill E. (2001). <i>Scaling to Very Very Large Corpora for Natural Language Disambiguation.</i></center>\n",
    "<br><br>\n",
    "No mundo ideal, todos os dados estariam disponíveis a qualquer momento, no melhor formato possível, sem erros ou omissões. No mundo real, a qualidade dos dados é muito variada e boa parte do tempo do analista de dados é despendido na aquisição e tratamento dos dados.\n",
    "<br><br>\n",
    "Sobre a aquisição de dados, uma dificuldade comumente encontrada é que eles não estão disponíveis em um formato diretamente consumível por um programa de computador para tarefas de análise. É muito comum que os dados que se quer analisar estejam em formatos originalmente pensados apenas para um usuário humano visualizá-los, como uma página web ou um documento do Word, sem terem sido criadas Application User Interfaces (APIs) para que outros programas pudessem tratar aquela informação de forma automática.\n",
    "<br><br>\n",
    "Mas, se os dados estão disponíveis para usuários humanos através de algum aplicativo, seria possível criar um programa que simulasse o acesso de um usuário e coletasse \"à força\" aqueles dados?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping\n",
    "*Data scraping*\\* (raspagem de dados) é o conjunto de técnicas de criação de códigos para coletar dados disponíveis em um formato pensado apenas no usuário final do sistema.\n",
    "<br><br>\n",
    "\\*Não confundir scraping (do verbo *to scrape*, pronuncia-se \"is<u>crei</u>pin\") com scrapping (do verbo  *to scrap*, pronuncia-se \"is<u>cré</u>pin\")\n",
    "<br><br>\n",
    "*Web scraping* é o data scraping feito em páginas web. No Python, as bibliotecas **BeautifulSoup** e **Requests** são bastante utilizadas para web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup\n",
    "A biblioteca *BeautifulSoup* facilita o acesso e a coleta de dados em arquivos HTML. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma página HTML\n",
    "html_exemplo = \"\"\"\n",
    "<html>\n",
    "<head>\n",
    "<title>Exemplo de página HTML</title>\n",
    "</head>\n",
    "<body>\n",
    "<p id=\"paragrafo1\">Olá!</p>\n",
    "<p id=\"paragrafo2\">Essa é uma página HTML. Clique <a href=\"https://html.spec.whatwg.org/\">aqui</a> para saber mais.</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "print(html_exemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um soup a partir da página HTML\n",
    "from bs4 import BeautifulSoup as bs\n",
    "soup = bs(html_exemplo, \"html.parser\")\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Embelezando o HTML\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acessando dos dados\n",
    "soup.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As propriedades do soup seguem a hierarquia dos elementos HTML\n",
    "soup.html.head.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mas também é possível recuperar um elemento sem explicitar os elementos hierarquicamente superiores\n",
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caso haja mais de um elemento do mesmo tipo, apenas o primeiro é retornado\n",
    "soup.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup.p é equivalente a soup.find(\"p\")\n",
    "soup.find(\"p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porém, o método find também permite outras formas de recuperar os elementos além do nome das tags\n",
    "soup.find(id=\"paragrafo2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(attrs={\"id\": \"paragrafo2\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz 1** Como pegar o elemento \"a\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O método find sempre retorna apenas um elemento\n",
    "# Para retornar uma lista de elementos que atendam a determinado critério, utilize o método find_all\n",
    "soup.find_all(\"p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A partir de um elemento, é possível recuperar o elemento pai e irmãos\n",
    "p1 = soup.find(\"p\")\n",
    "p1.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No caso dos elementos irmãos, deve-se atentar para as quebras de linhas e demais strings, que são consideradas elementos\n",
    "p1.next_sibling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1.next_sibling.next_sibling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next_sibling recupera os elementos irmãos a frente. Para pegar os elementos atrás, utilize previous_sibling\n",
    "p2 = soup.find_all(\"p\")[1]\n",
    "p2.previous_sibling.previous_sibling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz 2** O que retorna `p2.previous_sibling`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parent, next_sibling e previous_siblings consideram a hierarquia das tags no documento\n",
    "# Para recuperar elementos desconsiderando a hierarquia, utilize next_element e previous_element\n",
    "# As quebras de linhas e demais strings também são considerados elementos\n",
    "p2.next_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2.next_element.next_element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz 3** O que retorna `p2.next_element.next_element.next_element`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1.previous_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1.previous_element.previous_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O texto interno de um elemento está armazenado na propriedade text\n",
    "p1.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caso haja elementos filhos, a propriedade text concatena todos os textos internos\n",
    "p2.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# É possível recuperar o valor de algum atributo de uma tag\n",
    "link = soup.a\n",
    "link[\"href\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para pegar todos os atributos e valors, user a propriedade attrs\n",
    "link.attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requests\n",
    "A biblioteca *Requests* permite emitir requisições HTTP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazendo uma requisição\n",
    "# 200 = requisição bem-sucedida\n",
    "import requests\n",
    "r_contabeis = requests.get(\"https://www.contabeis.com.br/tabelas/ufesp/\")\n",
    "r_contabeis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Resgatando o código HTML da página\n",
    "print(r_contabeis.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um soup\n",
    "from bs4 import BeautifulSoup as bs\n",
    "soup_contabeis = bs(r_contabeis.text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperando dados\n",
    "# Utilize as ferramentas de desenvolvedor de um navegador para inspecionar a página\n",
    "uls = soup_contabeis.find_all(class_=\"itemList\")\n",
    "ul_2019 = uls[0]\n",
    "li_valor_2019 = ul_2019.find_all(\"li\")[1]\n",
    "print(\"O valor da UFESP de 2019 é \" + li_valor_2019.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz 4** O que retorna `list(li_valor_2019.next_siblings)[3].text`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Páginas dinâmicas\n",
    "Muitas páginas utilizam JavaScript para modificar o código HTML da página após ela ter sido renderizada pelo navegador.\n",
    "A biblioteca Requests **não** executa JavaScript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_sefaz = requests.get(\"https://portal.fazenda.sp.gov.br/Paginas/Indices.aspx\")\n",
    "print(\"UFESP\" in r_contabeis.text)\n",
    "print(\"UFESP\" in r_sefaz.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em páginas dinâmicas, uma alternativa para coletar os dados é utilizar as ferramentas de desenvolvedor de um navegador e analisar o funcionamento da página para identificar de onde vem a informação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Após utilizarmos as ferramentas de desenvolvedor para inspecionar o tráfego de requisições\n",
    "# foi possível identificar uma endereço que retorna um XML com as informações desejadas\n",
    "r_indices_sefaz = requests.get(\n",
    "    \"https://portal.fazenda.sp.gov.br/_api/Web/Lists/getByTitle('Indices')/items?$Select=Title,Valor\"\n",
    ")\n",
    "soup_indices = bs(r_indices_sefaz.text, \"html.parser\") # BeautifulSoup também trabalha com XML\n",
    "print(soup_indices.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_valor_list = soup_indices.find_all(\"d:valor\")\n",
    "d_valor_2019 = d_valor_list[-1]\n",
    "print(\"O valor da UFESP de 2019 é \" + d_valor_2019.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extração de dados de arquivos de Word\n",
    "Os arquivos de Word (\\*.docx) possuem um estrutura em XML. Portanto, são passíveis de serem manipulados pela biblioteca BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "with zipfile.ZipFile(\"documento.docx\", 'r') as zfp:\n",
    "    with zfp.open('word/document.xml') as doc_xml:\n",
    "        docx_soup = bs(doc_xml.read(), 'xml')\n",
    "\n",
    "print(docx_soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperando dados\n",
    "docx_soup.find_all(\"w:t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz 5** O que retorna `docx_soup.text`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extração de dados de arquivos PDF\n",
    "Arquivos PDF são menos estruturados para extração de texto e as informações coletadas estão sujeitas a um saneamento pois é comum surgirem caracteres estranhos em meio ao texto. Porém, a extração do texto \"bruto\" é uma tarefa fácil de ser realizada com a biblioteca *PyPDF2*.\n",
    "\n",
    "**Atenção:** a biblioteca PyPDF2 não faz parte do conjunto de pacotes padrão do Anaconda e deve ser instalada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "pdf_file = open('documento.pdf', 'rb')\n",
    "pdf_reader = PyPDF2.PdfFileReader(pdf_file)\n",
    "page = pdf_reader.getPage(0) # Contagem de páginas começa no 0\n",
    "text = page.extractText()\n",
    "print(repr(text))\n",
    "pdf_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando expressões regulares para recuperar as etapas do procedimento de recuperação de senha indo pessoalmente\n",
    "import re\n",
    "\n",
    "re.split(\"Posto Fiscal\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trecho = re.split(\"Posto Fiscal\", text)[1]\n",
    "trecho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trecho = re.split(\"ATENÇÃO!\", trecho)[0]\n",
    "trecho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etapas = re.split(\"\\n\\w\\)\", trecho)\n",
    "etapas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etapas = etapas[1:]\n",
    "etapas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etapas = [etapa.strip() for etapa in etapas]\n",
    "etapas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selenium\n",
    "O Selenium é uma biblioteca que permite a execução automática de comandos em um navegador. Originalmente criado para automatização de testes em páginas web, ele pode ser utilizado para realização de web scraping quando não for possível analisar o funcionamento de uma página dinâmica.\n",
    "\n",
    "\n",
    "**Atenção:** a biblioteca Selenium não faz parte do conjunto de pacotes padrão do Anaconda e deve ser instalada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurando o WebDriver do Chrome\n",
    "from selenium import webdriver\n",
    "\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\") # Baixe o Chrome WebDriver em https://chromedriver.chromium.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acessando uma site e colentando os dados\n",
    "import time\n",
    "\n",
    "driver.get(\"https://portal.fazenda.sp.gov.br/Paginas/Indices.aspx\")\n",
    "time.sleep(1) # Aguarde o carregamento dos dados\n",
    "td_valor_list = driver.find_elements_by_class_name(\"valor\")\n",
    "td_valor_2019 = td_valor_list[0]\n",
    "print(\"O valor da UFESP de 2019 é \" + td_valor_2019.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apesar de conveniente, o uso do Selenium tem algumas desvantagens em relação a análise da página dinâmica para coleta de dados:\n",
    "\n",
    "- Qualquer mudança na página pode impactar o script\n",
    "- É necessário se preocupar com as versões do navegador e do webdriver\n",
    "- Os scripts que utilizam Selenium serão mais lentos pois dependem do carregamento da página em um navegador"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "NBA_aula.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
